The features used to construct the agent’s value estimates are perhaps the most crucial part of a successful learning system. In this module we discuss two basic strategies for constructing features: (1) fixed basis that form an exhaustive partition of the input, and (2) adapting the features while the agent interacts with the world via Neural Networks and Backpropagation. In this week’s graded assessment you will solve a simple but infinite state prediction task with a Neural Network and TD learning.

Learning Objectives

Describe the difference between coarse coding and tabular representations
Explain the trade-off when designing representations between discrimination and generalization
Understand how different coarse coding schemes affect the functions that can be represented
Explain how tile coding is a (computationally?) convenient case of coarse coding
Describe how designing the tilings affects the resultant representation
Understand that tile coding is a computationally efficient implementation of coarse coding
Define a neural network
Define activation functions
Define a feedforward architecture
Understand how neural networks are doing feature construction
Understand how neural networks are a non-linear function of state
Understand how deep networks are a composition of layers
Understand the trade-off between learning capacity and challenges presented by deeper networks
Compute the gradient of a single hidden layer neural network
Understand how to compute the gradient for arbitrarily deep networks
Understand the importance of initialization for neural networks
Describe strategies for initializing neural networks
Describe optimization techniques for training neural networks