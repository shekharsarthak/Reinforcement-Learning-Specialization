This week you will learn how to estimate a value function for a given policy, when the number of states is much larger than the memory available to the agent. You will learn how to specify a parametric form of the value function, how to specify an objective function, and how estimating gradient descent can be used to estimate values from interaction with the world.

Learning Objectives

Understand how we can use parameterized functions to approximate value functions
Explain the meaning of linear value function approximation
Recognize that the tabular case is a special case of linear value function approximation
Understand that there are many ways to parameterize an approximate value function
Understand what is meant by generalization and discrimination
Understand how generalization can be beneficial
Explain why we want both generalization and discrimination from our function approximation
Understand how value estimation can be framed as a supervised learning problem
Recognize not all function approximation methods are well suited for reinforcement learning
Understand the mean-squared value error objective for policy evaluation
Explain the role of the state distribution in the objective
Understand the idea behind gradient descent and stochastic gradient descent
Outline the gradient Monte Carlo algorithm for value estimation
Understand how state aggregation can be used to approximate the value function
Apply Gradient Monte-Carlo with state aggregation
Understand the TD-update for function approximation
Highlight the advantages of TD compared to Monte-Carlo
Outline the Semi-gradient TD(0) algorithm for value estimation
Understand that TD converges to a biased value estimate
Understand that TD converges much faster than Gradient Monte Carlo
Derive the TD-update with linear function approximation
Understand that tabular TD(0) is a special case of linear semi-gradient TD(0)
Highlight the advantages of linear value function approximation over nonlinear
Understand the fixed point of linear TD learning
Describe a theoretical guarantee on the mean squared value error at the TD fixed point