Once the problem is formulated as an MDP, finding the optimal policy is more efficient when using value functions. This week, you will learn the definition of policies and value functions, as well as Bellman equations, which is the key technology that all of our algorithms will use.

Learning Objectives

Recognize that a policy is a distribution over actions for each possible state
Describe the similarities and differences between stochastic and deterministic policies
Generate examples of valid policies for a given MDP
Describe the roles of state-value and action-value functions in reinforcement learning
Describe the relationship between value functions and policies
Create examples of valid value functions for a given MDP
Derive the Bellman equation for state-value functions
Derive the Bellman equation for action-value functions
Understand how Bellman equations relate current and future values
Use the Bellman equations to compute value functions
Define an optimal policy
Understand how a policy can be at least as good as every other policy in every state
Identify an optimal policy for given MDPs
Derive the Bellman optimality equation for state-value functions
Derive the Bellman optimality equation for action-value functions
Understand how the Bellman optimality equations relate to the previously introduced Bellman equations
Understand the connection between the optimal value function and optimal policies
Verify the optimal value function for given MDPs