For the first week of this course, you will learn how to understand the exploration-exploitation trade-off in sequential decision-making, implement incremental algorithms for estimating action-values, and compare the strengths and weaknesses to different algorithms for exploration. For this weekâ€™s graded assessment, you will implement and test an epsilon-greedy agent.

Learning Objectives

Define reward
Understand the temporal nature of the bandit problem
Define k-armed bandit
Define action-values
Define action-value estimation methods
Define exploration and exploitation
Select actions greedily using an action-value function
Define online learning
Understand a simple online sample-average action-value estimation method
Define the general online update equation
Understand why we might use a constant stepsize in the case of non-stationarity
Define epsilon-greedy
Compare the short-term benefits of exploitation and the long-term benefits of exploration
Understand optimistic initial values
Describe the benefits of optimistic initial values for early exploration
Explain the criticisms of optimistic initial values
Describe the upper confidence bound action selection method
Define optimism in the face of uncertainty