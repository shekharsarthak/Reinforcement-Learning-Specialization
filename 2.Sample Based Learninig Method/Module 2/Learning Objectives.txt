This week you will learn how to estimate value functions and optimal policies, using only sampled experience from the environment. This module represents our first step toward incremental learning methods that learn from the agentâ€™s own interaction with the world, rather than a model of the world. You will learn about on-policy and off-policy methods for prediction and control, using Monte Carlo methods---methods that use sampled returns. You will also be reintroduced to the exploration problem, but more generally in RL, beyond bandits.

Learning Objectives

Understand how Monte-Carlo methods can be used to estimate value functions from sampled interaction
Identify problems that can be solved using Monte-Carlo methods
Use Monte-Carlo prediction to estimate the value function for a given policy.
Estimate action-value functions using Monte-Carlo
Understand the importance of maintaining exploration in Monte-Carlo algorithms
Understand how to use Monte-Carlo methods to implement a GPI algorithm
Apply Monte-Carlo with exploring starts to solve an MDP
Understand why Exploring Starts can be problematic in real problems
Describe an alternative exploration method for Monte-Carlo control
Understand how off-policy learning can help deal with the exploration problem
Produce examples of target policies and examples of behavior policies.
Understand importance sampling
Use importance sampling to estimate the expected value of a target distribution using samples from a different distribution.
Understand how to use importance sampling to correct returns
Understand how to modify the Monte-Carlo prediction algorithm for off-policy learning.