Up until now, you might think that learning with and without a model are two distinct, and in some ways, competing strategies: planning with Dynamic Programming verses sample-based learning via TD methods. This week we unify these two strategies with the Dyna architecture. You will learn how to estimate the model from data and then use this model to generate hypothetical experience (a bit like dreaming) to dramatically improve sample efficiency compared to sample-based methods like Q-learning. In addition, you will learn how to design learning systems that are robust to inaccurate models.

Learning Objectives

Describe what a model is and how they can be used
Classify models as distribution models or sample models
Identify when to use a distribution model or sample model
Describe the advantages and disadvantages of sample models and distribution models
Explain why sample models can be represented more compactly than distribution models
Explain how planning is used to improve policies
Describe random-sample one-step tabular Q-planning
Recognize that direct RL updates use experience from the environment to improve a policy or value function
Recognize that planning updates use experience from a model to improve a policy or value function
Describe how both direct RL and planning updates can be combined through the Dyna architecture
Describe the Tabular Dyna-Q algorithm
Identify the direct RL and planning updates in Tabular Dyna-Q
Identify the model learning and search control components of Tabular Dyna-Q
Describe how learning from both direct and simulated experience impacts performance
Describe how simulated experience can be useful when the model is accurate
Identify ways in which models can be inaccurate
Explain the effects of planning with an inaccurate model
Describe how Dyna can plan successfully with a partially inaccurate model
Explain how model inaccuracies produce another exploration-exploitation trade-off
Describe how Dyna-Q+ proposes a way to address this trade-off